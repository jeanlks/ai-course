{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23272259-9639-469e-a35c-b331d49b8b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jmonte/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980a1f1c-155f-459a-9cca-bd295c3973ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''\n",
    "\n",
    "Natural language processing has its roots in the 1950s.[1] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n",
    "Symbolic NLP (1950s â€“ early 1990s)\n",
    "\n",
    "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n",
    "\n",
    "    1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the first statistical machine translation systems were developed.\n",
    "    1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[4]\n",
    "\n",
    "    1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).\n",
    "    1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53f4cb0-9d04-49c3-a40c-1cb4f6a30ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64702819-7ddc-4307-9f0d-05c14fb6310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5f4667f-a570-446d-aeb1-c895ea640090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0192f4f6-c84b-4291-944f-fa8692f9eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = sent_tokenize(corpus)\n",
    "sentences2 = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c56f684a-c962-47fd-9df2-88f9313450b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nNatural language processing has its roots in the 1950s.',\n",
       " '[1] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.',\n",
       " 'The proposed test includes a task that involves the automated interpretation and generation of natural language.',\n",
       " \"Symbolic NLP (1950s â€“ early 1990s)\\n\\nThe premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\",\n",
       " '1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.',\n",
       " 'The authors claimed that within three or five years, machine translation would be a solved problem.',\n",
       " '[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.',\n",
       " 'Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the first statistical machine translation systems were developed.',\n",
       " '1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.',\n",
       " 'Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.',\n",
       " 'When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".',\n",
       " \"Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.\",\n",
       " '[4]\\n\\n    1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.',\n",
       " 'Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).',\n",
       " 'During this time, the first chatterbots were written (e.g., PARRY).',\n",
       " '1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP.',\n",
       " 'Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory).',\n",
       " 'Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky.',\n",
       " 'An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.',\n",
       " '[7]']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cf98fe1-f584-4997-a5f2-233ebad7f627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "178f7ac3-1362-490b-9321-0fd2d450b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stopwords and filter then stemming\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1fc6444-b9f3-49df-b95f-5c1f77d59928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process root 1950 .',\n",
       " '[ 1 ] alreadi 1950 , alan ture publish articl titl `` comput machineri intellig `` propo call ture test criterion intellig , though time articul problem separ artifici intellig .',\n",
       " 'propo test includ task involv autom interpret gener natur languag .',\n",
       " \"symbol nlp ( 1950 â€“ ear 1990 ) premi symbol nlp well-summar john searl 's chine room experi : given collect rule ( e.g . , chine phrasebook , question match answer ) , comput emul natur languag understand ( nlp task ) appli rule data confront .\",\n",
       " '1950 : georgetown experi 1954 involv fulli automat translat sixti russian sentenc english .',\n",
       " 'author claim within three five year , machin translat would solv problem .',\n",
       " '[ 2 ] howev , real progress much slower , alpac report 1966 , found ten year research fail fulfil expect , fund machin translat dramat reduc .',\n",
       " 'littl research machin translat conduct america ( though research continu elsewh , japan europ [ 3 ] ) late 1980 first statist machin translat system develop .',\n",
       " '1960 : notabl success natur languag process system develop 1960 shrdlu , natur languag system work restrict `` block world `` restrict vocabulari , eliza , simul rogerian psychotherapist , written joseph weizenbaum 1964 1966 .',\n",
       " 'use almost inform human thought emot , eliza sometim provid startl human-lik interact .',\n",
       " '`` patient `` exceed small knowledg base , eliza might provid gener respon , exampl , respond `` head hurt `` `` whi say head hurt ? `` .',\n",
       " \"ross quillian 's success work natur languag demonstr vocabulari twenti word , would fit comput memori time .\",\n",
       " '[ 4 ] 1970 : dure 1970 , mani programm began write `` conceptu ontolog `` , structur real-world inform computer-understand data .',\n",
       " 'exampl margi ( schank , 1975 ) , sam ( cullingford , 1978 ) , pam ( wilenski , 1978 ) , talespin ( meehan , 1976 ) , qualm ( lehnert , 1977 ) , polit ( carbonel , 1979 ) , plot unit ( lehnert 1981 ) .',\n",
       " 'dure time , first chatterbot written ( e.g . , parri ) .',\n",
       " '1980 : 1980 ear 1990 mark heyday symbol method nlp .',\n",
       " 'focu area time includ research rule-ba par ( e.g . , develop hpsg comput oper gener grammar ) , morpholog ( e.g . , two-level morpholog [ 5 ] ) , semant ( e.g . , lesk algorithm ) , refer ( e.g . , within center theori [ 6 ] ) area natur languag understand ( e.g . , rhetor structur theori ) .',\n",
       " 'line research continu , e.g . , develop chatterbot racter jabberwacki .',\n",
       " 'import develop ( eventu led statist turn 1990 ) rise import quantit evalu period .',\n",
       " '[ 7 ]']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d8749ee-c001-481a-9d8c-58f63d4890a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2241601c-95f9-4cdb-b6e3-8b96baa663e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(SnowballStemmer.languages)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9021feef-9933-4618-a59c-878337e4e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9a35408-ec79-4358-ad24-2d350f6b8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stopwords and filter then stemming\n",
    "for i in range(len(sentences2)):\n",
    "    words=nltk.word_tokenize(sentences2[i])\n",
    "    words=[stemmer2.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    sentences2[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b5ff9-2743-478d-8a50-52244d02879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
